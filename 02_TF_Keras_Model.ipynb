{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training a Simple Artificial Neural Network with TensorFlow\n",
        "This notebook provides a **step-by-step introduction** to the core concepts of training an Artificial Neural Network (ANN) using basic TensorFlow operations. Your task is to **fill in the missing code blocks** based on the accompanying theoretical explanation and hints to complete the learning process.\n",
        "\n",
        "We will incrementally build a simple network, focusing on the fundamental operations:\n",
        "1.  **Forward Pass** (Initialization and Prediction)\n",
        "2.  **Backward Pass** (Gradient Calculation and Basic Weight Update)\n",
        "3.  **Adding Bias**\n",
        "4.  **Adding Activation Functions**\n",
        "5.  **Adding Momentum**\n",
        "6.  **Keras Implementation** (Comparing manual vs. high-level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "    \"\"\"Calculates the Mean Squared Error (MSE) for a single sample.\"\"\"\n",
        "    # Using tf.reduce_mean to handle multiple samples/batches \n",
        "    return tf.reduce_mean((y_true - y_pred) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Forward Pass (No Bias, No Activation)\n",
        "\n",
        "The **Forward Pass** is the process of calculating the network's output prediction. In this initial simplified model, the output of each layer is simply the **weighted sum** of its inputs: $h = x W^T$. \n",
        "\n",
        "**Task:** Use `tf.matmul` and `tf.transpose` to calculate the hidden layer output (`h1`) and the final prediction (`y_pred`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input tensor (1 sample, 2 features)\n",
        "x = tf.constant([[2.0, 3.0]], dtype=tf.float32)  # shape (1, 2)\n",
        "\n",
        "# Initial weights (must be tf.Variable for gradient tracking)\n",
        "w1 = tf.Variable([[0.11, 0.21], [0.12, 0.08]], dtype=tf.float32)  # Hidden layer weights: shape (2, 2) (2 features -> 2 neurons)\n",
        "w2 = tf.Variable([[0.14, 0.15]], dtype=tf.float32)  # Output layer weights: shape (1, 2) (2 hidden neurons -> 1 output neuron)\n",
        "\n",
        "# Target output\n",
        "y_true = tf.constant([[1.0]], dtype=tf.float32)\n",
        "\n",
        "# Forward pass calculation:\n",
        "# --------------------------------------------------------------------------------\n",
        "# TODO: Calculate the hidden layer output (h1)\n",
        "# Hint: h1 = x @ w1.T\n",
        "# h1 = \n",
        "\n",
        "# TODO: Calculate the final prediction (y_pred)\n",
        "# Hint: y_pred = h1 @ w2.T\n",
        "# y_pred = \n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "# Show results\n",
        "print(\"Prediction value:\", y_pred.numpy())\n",
        "print(\"Loss function (Initial MSE):\", loss_function(y_true, y_pred).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Backward Pass and Basic Weight Update\n",
        "\n",
        "The **Backward Pass** uses `tf.GradientTape` to calculate the gradient of the loss with respect to all `tf.Variable`s. The weights are then updated using the Gradient Descent rule:\n",
        "$$\n",
        "\\mathbf{W}_{new} = \\mathbf{W}_{old} - \\eta \\times \\nabla\n",
        "$$ \n",
        "where $\\eta$ is the learning rate (`lr`).\n",
        "\n",
        "**Task:** Complete the weight update loop using the `var.assign_sub()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-initialize weights to demonstrate the update\n",
        "w1 = tf.Variable([[0.11, 0.21], [0.12, 0.08]], dtype=tf.float32)\n",
        "w2 = tf.Variable([[0.14, 0.15]], dtype=tf.float32)\n",
        "lr = 0.01\n",
        "\n",
        "print(\"Initial w1:\\n\", w1.numpy())\n",
        "\n",
        "# Backward pass to calculate gradients\n",
        "with tf.GradientTape() as tape:\n",
        "    h1 = tf.matmul(x, tf.transpose(w1))\n",
        "    y_pred = tf.matmul(h1, tf.transpose(w2))\n",
        "    loss = loss_function(y_true, y_pred)\n",
        "    \n",
        "grads = tape.gradient(loss, [w1, w2])\n",
        "\n",
        "# Weight update using Gradient Descent\n",
        "# --------------------------------------------------------------------------------\n",
        "for var, grad in zip([w1, w2], grads):\n",
        "    # TODO: Implement the weight update W = W - lr * dL/dW\n",
        "    # Hint: Use var.assign_sub() with the learning rate (lr) and the gradient (grad)\n",
        "\n",
        "    pass # Delete this line once implemented\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\nLoss before update:\", loss.numpy())\n",
        "print(\"\\nGradient for w1:\\n\", grads[0].numpy())\n",
        "print(\"\\nUpdated w1 (after 1 step):\\n\", w1.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practice: A Training Loop\n",
        "\n",
        "Run a complete training loop using the code developed previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-initialize weights for the training loop\n",
        "w1 = tf.Variable(tf.random.normal([2, 2]), dtype=tf.float32)\n",
        "w2 = tf.Variable(tf.random.normal([1, 2]), dtype=tf.float32)\n",
        "\n",
        "x = tf.constant([[0., 0.],[0., 1.],[1., 0.],[1., 1.]], dtype=tf.float32)\n",
        "y_true = tf.constant([[0.], [0.5], [0.5], [1.]], dtype=tf.float32)\n",
        "lr = 0.01\n",
        "\n",
        "print(\"Starting simple linear training\")\n",
        "for step in range(500):\n",
        "    # --------------------------------------------------------------------------------\n",
        "    with tf.GradientTape() as tape:\n",
        "        # TODO: Implement the Forward Pass (Linear)\n",
        "        # h1 = \n",
        "        # y_pred = \n",
        "        \n",
        "        loss = loss_function(y_true, y_pred)\n",
        "        \n",
        "    grads = tape.gradient(loss, [w1, w2])\n",
        "    \n",
        "    # TODO: Implement the Weight Update (Gradient Descent)\n",
        "    # for var, grad in zip(...):\n",
        "    #     var.assign_sub(...)\n",
        "    # --------------------------------------------------------------------------------\n",
        "    \n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step}: Loss = {loss.numpy():.4f}\")\n",
        "\n",
        "print(\"\\nFinal predictions:\\n\", y_pred.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Adding Bias\n",
        "\n",
        "The **Bias term** $\\mathbf{b}$ enables the model to learn functions that do not pass through the origin. The layer output equation is now:\n",
        "$$\\mathbf{h} = \\mathbf{x} \\mathbf{W}^T + \\mathbf{b}$$\n",
        "\n",
        "**Task:** Modify the forward pass to include the bias terms (`b1`, `b2`). Remember to include all four variables in `trainable_variables` for gradient calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train data\n",
        "x = tf.constant([[0., 0.],[0., 1.],[1., 0.],[1., 1.]], dtype=tf.float32)\n",
        "y_true = tf.constant([[-1.], [0.], [0.], [1.]], dtype=tf.float32)\n",
        "\n",
        "# Re-initialize weights and biases\n",
        "w1 = tf.Variable(tf.random.normal([2, 2]), dtype=tf.float32)\n",
        "b1 = tf.Variable(tf.zeros([2]), name='b1', dtype=tf.float32) # Bias for the hidden layer\n",
        "w2 = tf.Variable(tf.random.normal([1, 2]), dtype=tf.float32)\n",
        "b2 = tf.Variable(tf.zeros([1]), name='b2', dtype=tf.float32) # Bias for the output layer\n",
        "\n",
        "lr = 0.01\n",
        "# --------------------------------------------------------------------------------\n",
        "# TODO: Initialize trainable_variables to include biases\n",
        "# trainable_variables = []\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "print(\"Starting training with Bias\")\n",
        "for step in range(500):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # --------------------------------------------------------------------------------\n",
        "        # TODO: Add bias to the hidden layer output (tf.matmul + b1)\n",
        "        # h1 = \n",
        "        # TODO: Add bias to the output layer output (tf.matmul + b2)\n",
        "        # y_pred = \n",
        "        # --------------------------------------------------------------------------------\n",
        "        loss = loss_function(y_true, y_pred)\n",
        "        \n",
        "    grads = tape.gradient(loss, trainable_variables)\n",
        "    \n",
        "    for var, grad in zip(trainable_variables, grads):\n",
        "        if grad is not None:\n",
        "            var.assign_sub(lr * grad)\n",
        "            \n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step}: Loss = {loss.numpy():.4f}\")\n",
        "\n",
        "print(\"\\nFinal predictions (XOR targets, linear model with bias):\\n\", y_pred.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Adding Activation Functions\n",
        "\n",
        "To solve a non-linear problem, we must introduce **Activation Functions**. We will use the **Rectified Linear Unit (ReLU)** activation for the hidden layer:\n",
        "$$\\text{ReLU}(z) = \\max(0, z)$$\n",
        "\n",
        "**Task:** Apply the ReLU function using `tf.nn.relu()` to the hidden layer output (`h1_pre_activation`) before passing it to the output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train data\n",
        "x = tf.constant([[0., 0.],[0., 1.],[1., 0.],[1., 1.]], dtype=tf.float32)\n",
        "y_true = tf.constant([[0.], [1.], [1.], [0.]], dtype=tf.float32)\n",
        "\n",
        "# Re-initialize weights and biases for a fresh start with the new component\n",
        "w1 = tf.Variable(tf.random.normal([2, 2]), dtype=tf.float32)\n",
        "b1 = tf.Variable(tf.zeros([2]), dtype=tf.float32) \n",
        "w2 = tf.Variable(tf.random.normal([1, 2]), dtype=tf.float32)\n",
        "b2 = tf.Variable(tf.zeros([1]), dtype=tf.float32) \n",
        "\n",
        "lr = 0.01\n",
        "trainable_variables = [w1, b1, w2, b2]\n",
        "\n",
        "print(\"Starting training with Bias AND ReLU Activation (XOR data)...\")\n",
        "for step in range(2000): # Increased steps for non-linear problem\n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "        # Hidden Layer: Weighted Sum + Bias\n",
        "        h1_pre_activation = tf.matmul(x, tf.transpose(w1)) + b1 \n",
        "        \n",
        "        # --------------------------------------------------------------------------------\n",
        "        # TODO: Apply Non-linear Activation (ReLU: tf.nn.relu) to the pre-activation output\n",
        "        # h1 = \n",
        "        # --------------------------------------------------------------------------------\n",
        "        \n",
        "        # Output Layer\n",
        "        y_pred = tf.matmul(h1, tf.transpose(w2)) + b2 \n",
        "\n",
        "        loss = loss_function(y_true, y_pred)\n",
        "\n",
        "    grads = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "    for var, grad in zip(trainable_variables, grads):\n",
        "        if grad is not None: \n",
        "            var.assign_sub(lr * grad)\n",
        "            \n",
        "    if step % 200 == 0:\n",
        "        print(f\"Step {step}: Loss = {loss.numpy():.4f}\")\n",
        "\n",
        "# Final prediction (recalculate with the learned non-linear model)\n",
        "final_h1_pre_activation = tf.matmul(x, tf.transpose(w1)) + b1\n",
        "final_h1 = tf.nn.relu(final_h1_pre_activation)\n",
        "final_output = tf.matmul(final_h1, tf.transpose(w2)) + b2\n",
        "\n",
        "print(\"\\nFinal predictions (XOR targets, non-linear model):\\n\", final_output.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Adding Momentum\n",
        "\n",
        "**Momentum** is an optimization technique that helps accelerate gradient descent by accumulating a velocity vector $\\mathbf{v}_{t}$ from previous steps. \n",
        "\n",
        "The update rule for a weight $\\mathbf{W}$ becomes:\n",
        "$$\\mathbf{v}_{t} = \\mu \\mathbf{v}_{t-1} + \\eta \\nabla J(\\mathbf{W}_{t})$$\n",
        "$$\\mathbf{W}_{t+1} = \\mathbf{W}_{t} - \\mathbf{v}_{t}$$\n",
        "where $\\mu$ is the momentum coefficient.\n",
        "\n",
        "**Task:** Implement the two-step momentum update inside the loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset weights and biases\n",
        "w1 = tf.Variable(tf.random.normal([2, 2]), dtype=tf.float32)\n",
        "b1 = tf.Variable(tf.zeros([2]), dtype=tf.float32)\n",
        "w2 = tf.Variable(tf.random.normal([1, 2]), dtype=tf.float32)\n",
        "b2 = tf.Variable(tf.zeros([1]), dtype=tf.float32)\n",
        "\n",
        "trainable_variables = [w1, b1, w2, b2]\n",
        "\n",
        "# Initialize Momentum 'Velocity' vectors to zero\n",
        "v_w1, v_b1 = tf.zeros_like(w1), tf.zeros_like(b1)\n",
        "v_w2, v_b2 = tf.zeros_like(w2), tf.zeros_like(b2)\n",
        "velocity_variables = [v_w1, v_b1, v_w2, v_b2]\n",
        "\n",
        "# Hyperparameters\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "print(\"Starting training with Momentum (and ReLU)...\")\n",
        "for step in range(2000):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward Pass (with Bias and ReLU)\n",
        "        h1_pre_activation = tf.matmul(x, tf.transpose(w1)) + b1 \n",
        "        h1 = tf.nn.relu(h1_pre_activation) \n",
        "        y_pred = tf.matmul(h1, tf.transpose(w2)) + b2 \n",
        "        loss = loss_function(y_true, y_pred)\n",
        "\n",
        "    grads = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "    # Apply updates with Momentum\n",
        "    new_velocity_variables = []\n",
        "    # --------------------------------------------------------------------------------\n",
        "    for var, grad, vel in zip(trainable_variables, grads, velocity_variables):\n",
        "        if grad is not None:\n",
        "            # TODO 1: Calculate new velocity vector: v_t = mu * v_{t-1} + eta * grad\n",
        "            # new_vel = \n",
        "            \n",
        "            # TODO 2: Update weight: W = W - V_t (using var.assign_sub)\n",
        "            # var.assign_sub(...)\n",
        "            \n",
        "            new_velocity_variables.append(new_vel)\n",
        "        else:\n",
        "            new_velocity_variables.append(vel) # keep old velocity if no gradient\n",
        "            \n",
        "    velocity_variables = new_velocity_variables\n",
        "    # --------------------------------------------------------------------------------\n",
        "    \n",
        "    if step % 200 == 0:\n",
        "        print(f\"Step {step}: Loss = {loss.numpy():.4f}\")\n",
        "\n",
        "# Final prediction (recalculate with the learned non-linear model)\n",
        "final_h1_pre_activation = tf.matmul(x, tf.transpose(w1)) + b1\n",
        "final_h1 = tf.nn.relu(final_h1_pre_activation)\n",
        "final_output = tf.matmul(final_h1, tf.transpose(w2)) + b2\n",
        "\n",
        "print(\"\\nFinal predictions (Momentum enabled):\\n\", final_output.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Using Keras for High-Level Implementation\n",
        "\n",
        "In practice, most deep learning tasks use TensorFlow's high-level API, Keras, which handles all the manual steps we performed (variables, gradients, updates, momentum, etc.) automatically. This section replicates the XOR problem using Keras.\n",
        "\n",
        "**Task:** Complete the model definition and the compiler call. Note that we are using the `tanh` activation here, which often performs better than ReLU for XOR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 1. Prepare Data (data is already defined as x and y_true)\n",
        "\n",
        "# 2. Build the Model (equivalent to our two-layer network)\n",
        "# --------------------------------------------------------------------------------\n",
        "# TODO: Define the hidden layer (2 neurons, 'tanh' activation, input shape (2,))\n",
        "# TODO: Define the output layer (1 neuron, default linear activation)\n",
        "model = Sequential([\n",
        "    # Dense(2, activation='tanh', input_shape=(2,)),\n",
        "    # Dense(...)\n",
        "])\n",
        "\n",
        "# 3. Compile the Model (Define Optimizer, Loss, and Metrics)\n",
        "# TODO: Use the Adam optimizer with a learning rate of 0.1\n",
        "model.compile(\n",
        "    # optimizer=,\n",
        "    loss='mse',\n",
        "    metrics=['mean_squared_error']\n",
        ")\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "# 4. Train the Model\n",
        "print(\"\\nStarting Keras Training\")\n",
        "history = model.fit(\n",
        "    x, y_true,\n",
        "    epochs=100, \n",
        "    verbose=1 # display (or not) output for every epoch\n",
        ")\n",
        "\n",
        "# 5. Evaluate and Print Results\n",
        "final_loss = history.history['loss'][-1]\n",
        "print(f\"Training Complete. Final Loss: {final_loss:.4f}\")\n",
        "\n",
        "final_predictions = model.predict(x)\n",
        "print(\"\\nFinal Keras Predictions (XOR targets):\\n\", final_predictions)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
