{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122cdc0f",
   "metadata": {},
   "source": [
    "# Compile model: Loss Functions, Optimizers, and Metrics in TensorFlow\n",
    "This notebook introduces three fundamental concepts in TensorFlow:\n",
    "- **Loss Functions**: Measure how far predictions are from true values.\n",
    "- **Optimizers**: Update model weights to minimize the loss.\n",
    "- **Metrics**: Evaluate model performance during training and testing.\n",
    "\n",
    "We will explore each concept with explanations and code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f25b6",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Before training a Keras model, we need to compile it. The `compile()` function tells TensorFlow how the model should learn. One of the most important part of this setup is the loss function.\n",
    "\n",
    "The loss function measures how far off the model’s predictions are from the actual targets. Training is all about reducing this value: the smaller the loss, the closer the predictions are to reality.\n",
    "\n",
    "Different problems call for different loss functions: regression tasks often use Mean Squared Error, binary classification uses Binary Crossentropy, and multi-class classification uses Categorical Crossentropy. Choosing the right loss is essential because it directly shapes how your model learns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5384960",
   "metadata": {},
   "source": [
    "### Loss Functions for Regression Problems\n",
    "\n",
    "Regression problems involve predicting continuous values, like house prices or temperature.\n",
    "\n",
    "- **Mean Squared Error (MSE):** This is the most common loss function for regression. It calculates the average of the squared differences between predictions and true values. Squaring the difference penalizes larger errors more heavily and ensures the result is always positive.\n",
    "$$\n",
    "MSE = \\frac{1}{N} \\sum_{i=1}^{N} \\big(y_{true,i} - y_{pred,i}\\big)^2\n",
    "$$\n",
    "\n",
    "Where N is the number of samples. You typically specify it in Keras using the string identifier `mean_squared_error` or `mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae3a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Sample data\n",
    "y_true = tf.constant([[1.], [0.], [1.]])\n",
    "y_pred = tf.constant([[0.9], [0.2], [0.8]])\n",
    "\n",
    "# --- Manual implementation with for loop ---\n",
    "mse_manual = 0.0\n",
    "N = y_true.shape[0]\n",
    "for i in range(N):\n",
    "    mse_manual += (float(y_true[i]) - float(y_pred[i]))**2\n",
    "mse_manual /= N\n",
    "print(\"Manual MSE:\", mse_manual)\n",
    "\n",
    "# --- TensorFlow/Keras implementation ---\n",
    "mse_fn = tf.keras.losses.MeanSquaredError()\n",
    "mse_value = mse_fn(y_true, y_pred)\n",
    "print(\"tf.keras MSE:\", mse_value.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c64b1",
   "metadata": {},
   "source": [
    "- **Mean Absolute Error (MAE):** This loss function calculates the average of the absolute differences between predictions and true values. Unlike MSE, MAE doesn't square the errors, making it less sensitive to outliers. If your dataset contains significant outliers that you don't want to dominate the loss, MAE might be a better choice.\n",
    "$$\n",
    "MAE = \\frac{1}{N} \\sum_{i=1}^{N} \\left| y_{true,i} - y_{pred,i} \\right|\n",
    "$$\n",
    "\n",
    "You can specify it using `mean_absolute_error` or `mae`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dcc18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Sample data\n",
    "y_true = tf.constant([[1.], [0.], [1.]])\n",
    "y_pred = tf.constant([[0.9], [0.2], [0.8]])\n",
    "\n",
    "# --- Manual implementation with for loop ---\n",
    "mae_manual = 0.0\n",
    "N = y_true.shape[0]\n",
    "for i in range(N):\n",
    "    mae_manual += abs(float(y_true[i]) - float(y_pred[i]))\n",
    "mae_manual /= N\n",
    "print(\"Manual MAE:\", mae_manual)\n",
    "\n",
    "# --- TensorFlow/Keras implementation ---\n",
    "mae_fn = tf.keras.losses.MeanAbsoluteError()\n",
    "mae_value = mae_fn(y_true, y_pred)\n",
    "print(\"tf.keras MAE:\", mae_value.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9f02f",
   "metadata": {},
   "source": [
    "### Loss Functions for Classification Problems\n",
    "\n",
    "Classification problems involve predicting a discrete class label, like identifying spam emails or classifying images.\n",
    "\n",
    "- **Binary Crossentropy:** Use this loss function for binary (two-class) classification problems. It measures the distance between the true probability distribution (e.g., [0, 1] or [1, 0]) and the predicted probability distribution. It expects the model's final layer to have a single output unit with a sigmoid activation function (outputting a probability between 0 and 1), and the target values should be 0 or 1. The formula for binary crossentropy for a single prediction is:\n",
    "$$\n",
    "Loss = - \\Big( y_{true} \\cdot \\log(y_{pred}) + (1 - y_{true}) \\cdot \\log(1 - y_{pred}) \\Big)\n",
    "$$\n",
    "\n",
    "The final loss is averaged over all samples. Specify it using the string `binary_crossentropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47791752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "# Example data: binary classification\n",
    "y_true = tf.constant([[1.], [0.], [1.]])\n",
    "y_pred = tf.constant([[0.9], [0.2], [0.8]])\n",
    "\n",
    "# --- Manual implementation ---\n",
    "bce_manual = 0.0\n",
    "N = y_true.shape[0]\n",
    "eps = 1e-7  # to avoid log(0)\n",
    "for i in range(N):\n",
    "    yt = float(y_true[i])\n",
    "    yp = float(y_pred[i])\n",
    "    bce_manual += -(yt * math.log(yp + eps) + (1 - yt) * math.log(1 - yp + eps))\n",
    "bce_manual /= N\n",
    "print(\"Manual Binary Crossentropy:\", bce_manual)\n",
    "\n",
    "# --- TensorFlow/Keras implementation ---\n",
    "bce_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "bce_value = bce_fn(y_true, y_pred)\n",
    "print(\"tf.keras BinaryCrossentropy:\", bce_value.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09fe15",
   "metadata": {},
   "source": [
    "\n",
    "- **Categorical Crossentropy:** This is the standard loss function for multi-class classification when your target labels are one-hot encoded. For example, if you have three classes, the targets might look like [1, 0, 0], [0, 1, 0], or [0, 0, 1]. It expects the model's final layer to have C output units (where C is the number of classes) and use a softmax activation function, which outputs a probability distribution across the classes. The formula for a single sample is:\n",
    "\n",
    "$$\n",
    "Loss = - \\sum_{c=1}^{C} y_{true,c} \\cdot \\log(y_{pred,c})\n",
    "$$\n",
    "\n",
    "Where C is the number of classes. The final loss is averaged over all samples. Specify it using the string `categorical_crossentropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010abf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data: 3-class classification, one-hot labels\n",
    "y_true = tf.constant([[1,0,0], [0,1,0], [0,0,1]], dtype=tf.float32)\n",
    "y_pred = tf.constant([[0.7,0.2,0.1], [0.1,0.8,0.1], [0.2,0.2,0.6]], dtype=tf.float32)\n",
    "\n",
    "# --- Manual implementation ---\n",
    "cce_manual = 0.0\n",
    "N = y_true.shape[0]\n",
    "eps = 1e-7\n",
    "for i in range(N):\n",
    "    for c in range(y_true.shape[1]):\n",
    "        cce_manual += -float(y_true[i][c]) * math.log(float(y_pred[i][c]) + eps)\n",
    "cce_manual /= N\n",
    "print(\"Manual Categorical Crossentropy:\", cce_manual)\n",
    "\n",
    "# --- TensorFlow/Keras implementation ---\n",
    "cce_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "cce_value = cce_fn(y_true, y_pred)\n",
    "print(\"tf.keras CategoricalCrossentropy:\", cce_value.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ffdfde",
   "metadata": {},
   "source": [
    "- **Sparse Categorical Crossentropy:** This loss function serves the same purpose as categorical crossentropy but is used when your target labels are provided as integers (e.g., 0, 1, 2 for three classes) rather than one-hot encoded vectors. This is often more convenient as it avoids the need to explicitly convert integer labels to one-hot vectors. The model output requirements (C units, softmax activation) remain the same as for categorical crossentropy. Specify it using `sparse_categorical_crossentropy`. This often saves memory and computation compared to using `categorical_crossentropy` with explicitly one-hot encoded labels, especially for a large number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d30299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data: 3-class classification, integer labels\n",
    "y_true = tf.constant([0, 1, 2])  # class indices\n",
    "y_pred = tf.constant([[0.7,0.2,0.1], [0.1,0.8,0.1], [0.2,0.2,0.6]], dtype=tf.float32)\n",
    "\n",
    "# --- Manual implementation ---\n",
    "scce_manual = 0.0\n",
    "N = y_true.shape[0]\n",
    "eps = 1e-7\n",
    "for i in range(N):\n",
    "    true_class = int(y_true[i])\n",
    "    scce_manual += -math.log(float(y_pred[i][true_class]) + eps)\n",
    "scce_manual /= N\n",
    "print(\"Manual Sparse Categorical Crossentropy:\", scce_manual)\n",
    "\n",
    "# --- TensorFlow/Keras implementation ---\n",
    "scce_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "scce_value = scce_fn(y_true, y_pred)\n",
    "print(\"tf.keras SparseCategoricalCrossentropy:\", scce_value.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c665315a",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "While basic gradient descent provides the foundation, several more sophisticated optimizers have been developed to improve convergence speed and stability. TensorFlow's Keras API provides easy access to many of them. Here are some of the most frequently used:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d9fdcd",
   "metadata": {},
   "source": [
    "### SGD (Stochastic Gradient Descent)\n",
    "\n",
    "This is the classic optimizer. Instead of calculating the gradient using the entire dataset (which is computationally expensive), SGD estimates the gradient using a small random subset of the data called a mini-batch. It is computationally efficient, simple concept. However, it can have noisy updates, potentially slow convergence, sensitive to learning rate choice, might get stuck in local minima or saddle points more easily than adaptive methods.\n",
    "\n",
    "Keras's SGD optimizer often includes enhancements:\n",
    "\n",
    "- **Momentum:** This introduces a \"velocity\" component. Updates accumulate momentum in a consistent direction, helping to accelerate convergence, especially through flat regions or shallow ravines, and dampening oscillations.\n",
    "- **Nesterov Momentum:** A slight variation of momentum that often provides faster convergence in practice. It calculates the gradient \"looking ahead\" slightly in the direction of the momentum update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic SGD\n",
    "sgd_optimizer_basic = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# SGD with momentum\n",
    "sgd_optimizer_momentum = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# SGD with Nesterov momentum\n",
    "sgd_optimizer_nesterov = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcea50",
   "metadata": {},
   "source": [
    "### Adam (Adaptive Moment Estimation)\n",
    "\n",
    "Adam is often the default choice for many deep learning tasks due to its effectiveness and relative ease of use. It's an adaptive learning rate optimizer, meaning it computes individual learning rates for different parameters. It does this by keeping track of exponentially decaying averages of past gradients (first moment, like momentum) and past squared gradients (second moment, capturing the variance).\n",
    "\n",
    "Generally, ADAM converges quickly and performs well on a wide range of problems. However, it can sometimes converge to suboptimal solutions compared to finely tuned SGD with momentum and it requires more memory to store the moving averages.\n",
    "\n",
    "Important hyperparameters include learning_rate, beta_1 (decay rate for the first moment), beta_2 (decay rate for the second moment), and epsilon (a small value to prevent division by zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24383201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer with default parameters (learning_rate=0.001)\n",
    "adam_optimizer_default = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Adam optimizer with a custom learning rate\n",
    "adam_optimizer_custom = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "# Adam optimizer with custom beta values\n",
    "adam_optimizer_betas = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6942d58",
   "metadata": {},
   "source": [
    "### RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "RMSprop is another adaptive learning rate method that also maintains a moving average of the squared gradients. It divides the learning rate by the square root of this average. This effectively adapts the learning rate per parameter, decreasing it for parameters with large gradients and increasing it for parameters with small gradients.\n",
    "\n",
    "- Pros: Works well in practice, particularly for recurrent neural networks (RNNs). Good alternative to Adagrad (which can suffer from learning rates becoming too small).\n",
    "- Cons: Less commonly used as a default than Adam, but still very effective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae0938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# RMSprop optimizer with default parameters (learning_rate=0.001)\n",
    "rmsprop_optimizer_default = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "# RMSprop optimizer with custom learning rate and momentum\n",
    "rmsprop_optimizer_custom = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, momentum=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950460d9",
   "metadata": {},
   "source": [
    "### Choosing an Optimizer\n",
    "\n",
    "Keras offers other optimizers like Adagrad, Adadelta, Adamax, and Nadam. While less frequently used as initial choices compared to Adam or SGD, they have specific properties that might be beneficial for certain types of data or network architectures (e.g., Adagrad for sparse data). Selecting the best optimizer often involves some experimentation:\n",
    "\n",
    "- Start with Adam: Its adaptive nature and generally strong performance make it an excellent starting point for most problems. Use the default learning rate (0.001) initially.\n",
    "- Try SGD with Momentum: If Adam doesn't yield satisfactory results or if you suspect it's converging too quickly to a poor minimum, try SGD with momentum (e.g., momentum=0.9). This often requires more careful tuning of the learning rate. You might need to experiment with values like 0.1, 0.01, 0.001.\n",
    "- Consider RMSprop: It's a solid alternative, especially if you encounter issues with Adam or are working with RNNs.\n",
    "- Learning Rate Schedules: Instead of a fixed learning rate, you can use learning rate schedules that decrease the learning rate over time during training. This can help achieve finer convergence later in the training process. Keras callbacks or tf.keras.optimizers.schedules can implement this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a47cc",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "When compiling a model in TensorFlow/Keras, you configure the learning process by specifying, loss function, optimizer and Metrics. Metrics do not directly influence weight updates, but they provide valuable feedback on how well the model is performing.\n",
    "\n",
    "The choice of metrics depends heavily on your specific machine learning task (e.g., classification, regression) and what aspects of performance are most important :\n",
    "- **Classification**: Accuracy, Precision, Recall, AUC\n",
    "- **Regression**: MAE, MSE, RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec692f95",
   "metadata": {},
   "source": [
    "### Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of lassification metrics\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[\n",
    "                  tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "                  tf.keras.metrics.Precision(name='precision'),\n",
    "                  tf.keras.metrics.Recall(name='recall'),\n",
    "                  tf.keras.metrics.AUC(name='auc')\n",
    "              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f458b",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "Measures the proportion of correct predictions.\n",
    "\n",
    "$$Accuracy = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}$$\n",
    "\n",
    "==> Accuracy can be misleading with imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3fdf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Sample data\n",
    "y_true = tf.constant([[1.], [0.], [1.], [0.]])\n",
    "y_pred = tf.constant([[1.], [1.], [1.], [0.]])\n",
    "\n",
    "# Using BinaryAccuracy metric\n",
    "metric = tf.keras.metrics.BinaryAccuracy()\n",
    "metric.update_state(y_true, y_pred)\n",
    "\n",
    "# Display result\n",
    "print(\"Accuracy:\", metric.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd532fc",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "Out of all predicted positives, how many are truly positive?\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "Useful when false positives are costly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Precision metric\n",
    "metric = tf.keras.metrics.Precision()\n",
    "metric.update_state(y_true, y_pred)\n",
    "\n",
    "# Display result\n",
    "print(\"Precision:\", metric.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d92f9f",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "Out of all actual positives, how many did the model correctly identify?\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Useful when false negatives are costly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee96ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Recall metric\n",
    "metric = tf.keras.metrics.Recall()\n",
    "metric.update_state(y_true, y_pred)\n",
    "\n",
    "# Display result\n",
    "print(\"Recall:\", metric.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2946fca",
   "metadata": {},
   "source": [
    "#### AUC (Area Under ROC Curve)\n",
    "Measures the ability of the model to distinguish between classes.\n",
    "- AUC = 1.0 → Perfect classifier\n",
    "- AUC = 0.5 → Random guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using AUC metric\n",
    "metric = tf.keras.metrics.AUC()\n",
    "metric.update_state(y_true, y_pred)\n",
    "\n",
    "# Display result\n",
    "print(\"AUC:\", metric.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c367a",
   "metadata": {},
   "source": [
    "### Regression metrics\n",
    "\n",
    "In regression problems, the goal is to predict a **continuous value**.  \n",
    "To evaluate how well our model performs, we use metrics such as:\n",
    "\n",
    "- **Mean Absolute Error (MAE)**\n",
    "- **Mean Squared Error (MSE)**\n",
    "- **Root Mean Squared Error (RMSE)**\n",
    "\n",
    "These metrics provide different perspectives on prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b921e",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- Represents the average absolute difference between true and predicted values.  \n",
    "- Easy to interpret since it is in the same units as the target variable.  \n",
    "- Less sensitive to outliers compared to MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2801ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example true and predicted values\n",
    "y_true = tf.constant([[3.0], [-0.5], [2.0], [7.0]])\n",
    "y_pred = tf.constant([[2.5], [0.0],  [2.0], [8.0]])\n",
    "\n",
    "# Compute MAE\n",
    "mae_metric = tf.keras.metrics.MeanAbsoluteError()\n",
    "mae_metric.update_state(y_true, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae_metric.result().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9663913",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- Penalizes larger errors more heavily due to squaring.  \n",
    "- Units are the square of the target variable.  \n",
    "- Commonly used as a loss function and monitoring metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MSE\n",
    "mse_metric = tf.keras.metrics.MeanSquaredError()\n",
    "mse_metric.update_state(y_true, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse_metric.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5b3dd",
   "metadata": {},
   "source": [
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{MSE}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- Same units as the target variable.  \n",
    "- More interpretable than MSE.  \n",
    "- Still penalizes large errors significantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f271ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RMSE\n",
    "rmse_metric = tf.keras.metrics.RootMeanSquaredError()\n",
    "rmse_metric.update_state(y_true, y_pred)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse_metric.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe22de6",
   "metadata": {},
   "source": [
    "#### Custom Metrics in TensorFlow\n",
    "\n",
    "Sometimes built-in metrics are not enough.   TensorFlow allows to define **custom metrics** either as simple Python functions or by subclassing `tf.keras.metrics.Metric`.\n",
    "\n",
    "This gives flexibility to measure exactly what matters for the problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metric function\n",
    "def custom_metric_function(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    mask = tf.where(y_true > 5, tf.constant(10.0, dtype=tf.float32), tf.constant(1.0, dtype=tf.float32))\n",
    "    diff = tf.abs(y_true - y_pred) * mask\n",
    "    return tf.reduce_mean(diff)\n",
    "\n",
    "mape_value = custom_metric_function(y_true, y_pred)\n",
    "print(\"Custom MAE:\", mape_value.numpy())\n",
    "\n",
    "# Put the custom function inside model definition\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='mse',\n",
    "#               metrics=['mae', custom_metric_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde5536",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "We now build a simple binary classification model with synthetic data.\n",
    "We will compile the model with a loss, optimizer, and metrics, then train and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2cc3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X = np.random.rand(1000, 3)\n",
    "y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(3,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model with loss, optimizer, and metrics\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ff328",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "After training, we can evaluate the model on new data and inspect metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44740a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "X_test = np.random.rand(200, 3)\n",
    "y_test = (X_test[:, 0] + X_test[:, 1] > 1).astype(int)\n",
    "\n",
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Loss, Accuracy, AUC:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe862e8f",
   "metadata": {},
   "source": [
    "## Optimizing Hyperparameters with Grid Search (Keras)\n",
    "\n",
    "We now use TensorFlow/Keras integrated with Scikit-learn's **Grid Search** functionality to automatically test various combinations of hyperparameters. This systematic approach ensures we find the best settings for the **learning rate** and the **number of neurons** in the hidden layer.\n",
    "\n",
    "We will use the **Adam optimizer**, which is an adaptive optimization algorithm that already incorporates momentum-like behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ad2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD \n",
    "from scikeras.wrappers import KerasClassifier \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare Data \n",
    "X_train = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=np.float32)\n",
    "Y_train = np.array([[0.], [1.], [1.], [0.]], dtype=np.float32)\n",
    "\n",
    "# 2. Function to create the Keras model\n",
    "def create_model(learning_rate=0.01, n_neurons=2, optimizer_name='Adam'):\n",
    "    \"\"\"Creates a neural network model with specified hyperparameters, including optimizer.\"\"\"\n",
    "    model = Sequential([\n",
    "        # Hidden layer uses Sigmoid activation\n",
    "        Dense(n_neurons, activation='sigmoid', input_shape=(2,)), \n",
    "        Dense(1) \n",
    "    ])\n",
    "    \n",
    "    # Select optimizer based on the parameter value\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        # Using SGD as an alternative optimizer\n",
    "        optimizer = SGD(learning_rate=learning_rate) \n",
    "    else:\n",
    "        # Fallback for safety\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mean_squared_error']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 3. Create the KerasClassifier wrapper\n",
    "# Note: We set a high default epoch count here, but the Grid Search will override it.\n",
    "keras_model = KerasClassifier(\n",
    "    model=create_model, \n",
    "    verbose=0, \n",
    "    loss='mse' \n",
    ")\n",
    "\n",
    "# 4. Define the Grid Search parameter space\n",
    "# PARAMETER FIX: 'epochs' and 'batch_size' are Scikit-learn parameters (no prefix needed).\n",
    "# 'model__' parameters target arguments of the create_model function.\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.1, 0.01, 0.001], \n",
    "    'model__n_neurons': [2, 4, 8],\n",
    "    'model__optimizer_name': ['Adam', 'SGD'],\n",
    "    'batch_size': [2, 4],\n",
    "    'epochs': [10, 20] # New parameter for tuning the number of training passes\n",
    "}\n",
    "\n",
    "# 5. Initialize and Run Grid Search\n",
    "print(\"\\n Starting Grid Search to find optimal hyperparameters (this may take a moment)...\")\n",
    "\n",
    "# Grid search will perform cross-validation (cv=2) over the training data\n",
    "grid = GridSearchCV(estimator=keras_model, param_grid=param_grid, cv=2, scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "\n",
    "# 6. Summarize Results\n",
    "print(\"\\n Grid Search Complete.\")\n",
    "\n",
    "print(f\"Best Loss (MSE, lower is better): {-grid_result.best_score_:.4f}\")\n",
    "\n",
    "# Clean up parameter names for display\n",
    "best_params = {k.replace('model__', ''): v for k, v in grid_result.best_params_.items()}\n",
    "print(f\"Best Hyperparameters: {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
