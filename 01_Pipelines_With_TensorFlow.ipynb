{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUtoed20cRJJ"
   },
   "source": [
    "# End-to-End TensorFlow Pipeline\n",
    "This notebook demonstrates a complete machine learning pipeline using only TensorFlow functionalities. It includes:\n",
    "- CSV data loading\n",
    "- Numeric and categorical preprocessing (including one-hot encoding)\n",
    "- Train/test split\n",
    "- Model definition and training\n",
    "- Saving and loading the model for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgZ9gjmPfSnK"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:14:22.930357Z",
     "iopub.status.busy": "2024-08-16T07:14:22.929977Z",
     "iopub.status.idle": "2024-08-16T07:14:25.332422Z",
     "shell.execute_reply": "2024-08-16T07:14:25.331708Z"
    },
    "id": "baYFZMW_bJHh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B87Rd1SOUv02"
   },
   "source": [
    "### Basic preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCrB2Jd-U0Vt"
   },
   "source": [
    "TensorFlow provides a suite of preprocessing layers under `tf.keras.layers` that allow you to transform input data directly within your model. These layers are:\n",
    "\n",
    "- Fully compatible with `tf.data` pipelines\n",
    "- Exportable with the model for deployment\n",
    "- Efficient and GPU/TPU-friendly\n",
    "\n",
    "We'll explore key preprocessing layers for numeric, categorical, and text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric Feature Normalization\n",
    "\n",
    "Use `Normalization()` to scale numeric inputs to zero mean and unit variance. This is essential for stable training of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:14:30.740616Z",
     "iopub.status.busy": "2024-08-16T07:14:30.740371Z",
     "iopub.status.idle": "2024-08-16T07:14:30.744251Z",
     "shell.execute_reply": "2024-08-16T07:14:30.743644Z"
    },
    "id": "H2WQpDU5VRk7"
   },
   "outputs": [],
   "source": [
    "# Sample numeric data\n",
    "data = np.array([[1.0], [2.0], [3.0], [4.0], [5.0]])\n",
    "\n",
    "# Create and adapt normalization layer\n",
    "normalizer = tf.keras.layers.Normalization()\n",
    "# Use the `Normalization.adapt` method to adapt the normalization layer to your data\n",
    "normalizer.adapt(data)\n",
    "\n",
    "# Apply normalization\n",
    "normalized = normalizer(data)\n",
    "print(\"Normalized output:\\n\", normalized.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discretization\n",
    "\n",
    "Use `Discretization` to convert continuous numeric values into discrete bins. This is useful for bucketing features like age or income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample numeric data\n",
    "values = tf.constant([[5.0], [15.0], [25.0], [35.0], [10.0], [20.0]])\n",
    "\n",
    "# Define bin boundaries\n",
    "discretizer = tf.keras.layers.Discretization(bin_boundaries=[10.0, 20.0, 30.0])\n",
    "\n",
    "# Apply discretization\n",
    "binned = discretizer(values)\n",
    "print(\"Discretized output:\\n\", binned.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGgEZE-7Vpt6"
   },
   "source": [
    "#### Categorical Encoding\n",
    "\n",
    "Use `StringLookup` and `CategoryEncoding` to convert string categories into one-hot or multi-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T07:14:30.747781Z",
     "iopub.status.busy": "2024-08-16T07:14:30.747272Z",
     "iopub.status.idle": "2024-08-16T07:14:30.763518Z",
     "shell.execute_reply": "2024-08-16T07:14:30.762920Z"
    },
    "id": "2WgOPIiOVpLg"
   },
   "outputs": [],
   "source": [
    "# Sample categorical data\n",
    "train_categories = tf.constant(['red', 'green', 'blue', 'green', 'red'])\n",
    "\n",
    "# String lookup\n",
    "# Try also ['int', 'one_hot']\n",
    "lookup = tf.keras.layers.StringLookup(output_mode='one_hot')\n",
    "lookup.adapt(train_categories)\n",
    "\n",
    "# Encoding values\n",
    "test_categories = tf.constant(['green', 'blue', 'red', 'black', ''])\n",
    "encoded_values = lookup(test_categories)\n",
    "\n",
    "# Convert to NumPy\n",
    "input_strings = test_categories.numpy().astype(str)\n",
    "encoded_array = encoded_values.numpy()\n",
    "\n",
    "# Build DataFrame\n",
    "df = pd.DataFrame(encoded_array)\n",
    "df.insert(0, 'Input Category', input_strings)\n",
    "\n",
    "# Display result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample categorical data\n",
    "train_categories = tf.constant([['red', 'green'], ['blue', 'green'], ['red', 'yellow']])\n",
    "\n",
    "# String lookup\n",
    "# Try also ['multi_hot', 'count']\n",
    "lookup = tf.keras.layers.StringLookup(output_mode='count')\n",
    "lookup.adapt(train_categories)\n",
    "\n",
    "# Encoding values\n",
    "test_categories = tf.constant([['red', ''], ['blue', 'green'], ['black', 'yellow']])\n",
    "encoded_values = lookup(test_categories)\n",
    "\n",
    "# Convert to NumPy\n",
    "input_strings = test_categories.numpy().astype(str)\n",
    "encoded_array = encoded_values.numpy()\n",
    "\n",
    "# Build DataFrame\n",
    "df = pd.DataFrame(encoded_array)\n",
    "for i in range(input_strings.shape[1]):\n",
    "    df.insert(loc=i, column=f'Input_{i+1}', value=input_strings[:, i])\n",
    "\n",
    "# Display result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hashing\n",
    "\n",
    "Use `Hashing` to convert strings into integer indices using a hash function. Useful for high-cardinality categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample string data\n",
    "words = tf.constant(['green', 'red', 'blue', 'black', 'yellow', 'white', 'magenta'])\n",
    "\n",
    "# Hashing layer\n",
    "hasher = tf.keras.layers.Hashing(num_bins=4)\n",
    "hashed = hasher(words)\n",
    "print(\"Hashed output:\\n\", hashed.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Vectorization\n",
    "\n",
    "Use `TextVectorization` to tokenize and vectorize raw text into integer sequences or n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "texts = tf.constant([\"TensorFlow is great\", \"Preprocessing is powerful\"])\n",
    "\n",
    "# Text vectorization layer\n",
    "vectorizer = tf.keras.layers.TextVectorization(output_mode='int', max_tokens=10)\n",
    "vectorizer.adapt(texts)\n",
    "\n",
    "# Vectorized output\n",
    "test_texts = tf.constant([\"TensorFlow is fun, great and powerful\", \"\"])\n",
    "vectorized = vectorizer(test_texts)\n",
    "print(\"Vectorized text:\\n\", vectorized.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training Models in TensorFlow\n",
    "\n",
    "This notebook demonstrates how to build, compile, train, evaluate, and save models using TensorFlow's high-level Keras API. We explore:\n",
    "\n",
    "- Sequential and Functional APIs\n",
    "- Model compilation and training\n",
    "- Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential API\n",
    "\n",
    "The Sequential API is ideal for simple stack-like models where each layer has one input and one output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional API\n",
    "\n",
    "Use the Functional API for models with multiple inputs/outputs or non-linear topology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(10,))\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(inputs)\n",
    "x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the Model\n",
    "\n",
    "Specify the optimizer, loss function, and metrics for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model\n",
    "\n",
    "Use `model.fit()` to train the model on your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dummy data\n",
    "X_train = np.random.rand(1000, 10)\n",
    "y_train = np.random.randint(0, 2, size=(1000,))\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate and Predict\n",
    "\n",
    "Use `model.evaluate()` and `model.predict()` for testing and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.random.rand(200, 10)\n",
    "y_test = np.random.randint(0, 2, size=(200,))\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc} -- Test loss: {loss}\")\n",
    "\n",
    "predictions = model.predict(X_test[:5])\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks and Early Stopping\n",
    "\n",
    "Use callbacks like `EarlyStopping` and `ModelCheckpoint` to control training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, validation_split=0.2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and Load Models\n",
    "\n",
    "Use `model.save()` and `tf.keras.models.load_model()` to persist models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.keras')\n",
    "loaded_model = tf.keras.models.load_model('my_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Training Loop\n",
    "\n",
    "For full control, use `GradientTape` to write your own training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for i in range(0, len(X_train), 32):\n",
    "        x_batch = X_train[i:i+32]\n",
    "        y_batch = y_train[i:i+32]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch, training=True)\n",
    "            loss = loss_fn(y_batch, logits)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.numpy():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrating Preprocessing into a Model\n",
    "\n",
    "Preprocessing layers can be part of the model itself, making it portable and deployment-ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing layers\n",
    "normalizer = tf.keras.layers.Normalization()\n",
    "normalizer.adapt(np.array([[1.0], [2.0], [3.0]]))\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile and summarize\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My first TF pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load CSV Data\n",
    "\n",
    "We use `tf.data.experimental.make_csv_dataset` to load structured data from a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['feature1', 'feature2', 'feature3', 'category', 'label']\n",
    "DEFAULTS = [0.0, 0.0, 0.0, '', 0]\n",
    "\n",
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern='data.csv',\n",
    "    batch_size=32,\n",
    "    column_names=CSV_COLUMNS,\n",
    "    column_defaults=DEFAULTS,\n",
    "    label_name='label',\n",
    "    num_epochs=1,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Elements of dataset:')\n",
    "# Try to display elements of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Define Preprocessing Layers\n",
    "\n",
    "We normalize numeric features and one-hot encode the categorical feature using Keras preprocessing layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "NUMERIC_FEATURES = ['feature1', 'feature2', 'feature3']\n",
    "CATEGORICAL_FEATURE = ['category']\n",
    "\n",
    "# Extract raw data to adapt layers\n",
    "def extract_features(ds):\n",
    "    numeric_data = []\n",
    "    categorical_data = []\n",
    "    for batch in ds: \n",
    "        features, _ = batch\n",
    "        numeric_data.append(tf.stack([features[feature] for feature in NUMERIC_FEATURES], axis=-1))\n",
    "        categorical_data.append(tf.stack([features[feature] for feature in CATEGORICAL_FEATURE], axis=-1))\n",
    "    return tf.concat(numeric_data, axis=0), tf.concat(categorical_data, axis=0)\n",
    "\n",
    "# Adaptation Functions\n",
    "\n",
    "def get_normalization_layer(numeric_tensor):\n",
    "    # Create Normalization layer for numeric features\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(numeric_tensor)\n",
    "    return normalizer\n",
    "\n",
    "def get_category_encoding_layer(category_tensor, output_mode='one_hot'):\n",
    "    # Create StringLookup + One-hot encoding for categorical feature\n",
    "    lookup = tf.keras.layers.StringLookup(output_mode='int')\n",
    "    lookup.adapt(category_tensor)\n",
    "    category_encoding_layer = tf.keras.layers.CategoryEncoding(num_tokens=lookup.vocabulary_size(), output_mode=output_mode)\n",
    "    \n",
    "    return lookup, category_encoding_layer\n",
    "\n",
    "# Adapt the Layers\n",
    "numeric_tensor, category_tensor = extract_features(dataset)\n",
    "\n",
    "normalizer = get_normalization_layer(numeric_tensor)\n",
    "\n",
    "lookup, encoder = get_category_encoding_layer(category_tensor)\n",
    "\n",
    "print(\"Preprocessing layers adapted successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Build Preprocessing Submodel\n",
    "\n",
    "We create a preprocessing model that transforms raw dictionary inputs into numeric tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Raw Feature Inputs\n",
    "# Inputs must match the structure (name and dtype) of the data yielded by your CSV dataset.\n",
    "numeric_inputs_list = []\n",
    "for name in NUMERIC_FEATURES:\n",
    "    # Numeric inputs are batched, so shape is (None, 1) or just (1,) if unbatched\n",
    "    numeric_inputs_list.append(tf.keras.Input(shape=(1,), name=name, dtype=tf.float32))\n",
    "\n",
    "cat_inputs_list = []\n",
    "for name in CATEGORICAL_FEATURE:\n",
    "    # Numeric inputs are batched, so shape is (None, 1) or just (1,) if unbatched\n",
    "    cat_inputs_list.append(tf.keras.Input(shape=(1,), name=name, dtype=tf.string))\n",
    "\n",
    "# Preprocessing\n",
    "numeric_inputs = tf.keras.layers.Concatenate()(numeric_inputs_list)\n",
    "cat_inputs = tf.keras.layers.Concatenate()(cat_inputs_list)\n",
    "normalized = normalizer(numeric_inputs)\n",
    "encoded_category = encoder(lookup(cat_inputs))\n",
    "\n",
    "all_features = tf.keras.layers.Concatenate()([normalized, encoded_category])\n",
    "\n",
    "# Add Dense Layers (Model Architecture)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(all_features)\n",
    "x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x) # Binary classification output\n",
    "\n",
    "# Create the final end-to-end model\n",
    "end_to_end_model = tf.keras.Model(inputs=numeric_inputs_list+cat_inputs_list, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Build and Compile Full Model\n",
    "\n",
    "We embed the preprocessing model into the full model so it becomes part of the saved graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "end_to_end_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nEnd-to-End Model Summary:\")\n",
    "end_to_end_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Prepare Dataset for Training\n",
    "\n",
    "We map the dataset to a dictionary format compatible with the model's input signature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_batch(features, label):\n",
    "    return (\n",
    "        {\n",
    "            'feature1': tf.expand_dims(features['feature1'], -1),\n",
    "            'feature2': tf.expand_dims(features['feature2'], -1),\n",
    "            'feature3': tf.expand_dims(features['feature3'], -1),\n",
    "            'category': tf.expand_dims(features['category'], -1),\n",
    "        },\n",
    "        label\n",
    "    )\n",
    "\n",
    "train_dataset = dataset.map(format_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Train the Model\n",
    "\n",
    "We train the model using the standard `fit()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Train the Model ---\n",
    "EPOCHS = 5\n",
    "\n",
    "print(f\"\\nStarting training for {EPOCHS} epochs on raw dataset...\")\n",
    "\n",
    "\n",
    "\n",
    "# We use the original 'dataset' as the training input\n",
    "history = end_to_end_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Save and Reload the Model\n",
    "\n",
    "We save the entire model including preprocessing layers and reload it for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "end_to_end_model.save('model/full_pipeline.keras')  # Saves preprocessing layers too\n",
    "\n",
    "# Reload\n",
    "loaded_model = tf.keras.models.load_model('model/full_pipeline.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Inference with Raw Dictionary Input\n",
    "\n",
    "We can now pass raw feature dictionaries directly to the reloaded model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\n",
    "    'feature1': tf.constant([[1.2]]),\n",
    "    'feature2': tf.constant([[2.3]]),\n",
    "    'feature3': tf.constant([[3.1]]),\n",
    "    'category': tf.constant([['green']])\n",
    "}\n",
    "\n",
    "prediction = loaded_model(sample)\n",
    "print(\"Prediction:\", prediction.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wuqj601Qw0Ml"
   },
   "source": [
    "## My first TF Lab: Building a TensorFlow Prediction Pipeline\n",
    "\n",
    "- Build a self-contained Titanic survival prediction pipeline utilizing the provided dataset and integrating all necessary data preparation and modeling logic.\n",
    "\n",
    "- Bonus: Export and serve the trained model using a simple FastAPI (or Flask) API"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "csv.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
